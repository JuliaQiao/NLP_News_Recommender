{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_pickle('df_raw.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn to csv so we can store locally and store as database in sql\n",
    "df_raw.to_csv('/Users/juliaqiao/Documents/Metis/Proj_4_storage/df_raw.csv')"
   ]
  },
  {
   "source": [
    "### Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['user', 'date', 'outlinks', 'content', 'url'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex the columns for easier viewing\n",
    "cols = df_raw.columns.tolist()\n",
    "\n",
    "cols.insert(2, cols.pop(cols.index('url')))\n",
    "\n",
    "df_raw= df_raw.reindex(columns= cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary value of key: username\n",
    "# handle = [d[0].get('username') for d in df_raw.user if d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    Takes in tweet and performs initial text cleaning/preprocessing.\n",
    "    \"\"\"\n",
    "    #make sure doc is string\n",
    "    tweet=str(tweet)\n",
    "    #lowercase-- not changing anything to lowercase yet due to proper nouns being very important. want to use Spacy to detect later.\n",
    "    #tweet = tweet.lower()\n",
    "    #get rid of urls\n",
    "    rem_url=re.sub(r'http\\S+', '', tweet)\n",
    "    #gets rid of @ tags\n",
    "    rem_tag = re.sub('@\\S+', '', rem_url)\n",
    "    #gets rid of # in hashtag but keeps content of hashtag\n",
    "    rem_hashtag = re.sub('#', '', rem_tag)\n",
    "    #gets rid of numbers\n",
    "    rem_num = re.sub('[0-9]+', '', rem_hashtag)\n",
    "    #gets rid of special characters\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', rem_num)\n",
    "\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean']=df_raw['content'].map(lambda x:preprocess(x))"
   ]
  },
  {
   "source": [
    "### Tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TreebankWordTokenizer()\n",
    "df_raw['clean'] = df_raw['clean'].apply(tt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'is',\n",
       " 'devastating',\n",
       " 'U.S.',\n",
       " 'restaurant',\n",
       " 'chains.',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'why',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'companies',\n",
       " 'should',\n",
       " 'see',\n",
       " 'the',\n",
       " 'light',\n",
       " 'of',\n",
       " 'day',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future.',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 266
    }
   ],
   "source": [
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    " ### Lemmatize "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x]) # Lemmatize every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'is',\n",
       " 'devastating',\n",
       " 'U.S.',\n",
       " 'restaurant',\n",
       " 'chains.',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'why',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'company',\n",
       " 'should',\n",
       " 'see',\n",
       " 'the',\n",
       " 'light',\n",
       " 'of',\n",
       " 'day',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future.',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "#run example\n",
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    "### Remove Stop Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes default stop_words\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'devastating',\n",
       " 'U.S.',\n",
       " 'restaurant',\n",
       " 'chains.',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'company',\n",
       " 'see',\n",
       " 'light',\n",
       " 'day',\n",
       " 'near',\n",
       " 'future.',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "#run example\n",
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    "We did not use a stemmer. Lemmatizer was better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Remove any leftover punctuation:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', '/', ',', '%', '!', ':', \"'s\", \"'ve\", '\"', '*', '‚Äú', \"‚Äô\", \"`\", '?', '‚Äù', \"``\", \"‚Äò\"] \n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3977     [Unfinished, puzzle, half-done, scarf, unopene...\n",
       "4883     [This, wa, decade, American, population, turne...\n",
       "9346     [In, latest, issue, Fiona, Apple, discus, conf...\n",
       "17766    [T., S., Eliot, helped, make, Faber, &, amp, ;...\n",
       "13586    [Around, world, nationalist, government, claim...\n",
       "16680    [Physical, books‚Äîwhich, ten, year, ago, many, ...\n",
       "15779    [At, least, million, move, around, again., Pub...\n",
       "13939    [Twitter, temporarily, blocked, Trump, campaig...\n",
       "2410      [specific, way, tech, address, systemic, racism]\n",
       "14063    [Planet, Fitness, ', revenue, fell, Q, attribu...\n",
       "5006     [Scores, civilian, still, killed, wounded, eve...\n",
       "14154    [The, overlap, Asia, though, minimal, give, Eu...\n",
       "18849    [This, year, celebrate, th, birthday, Walt, Wh...\n",
       "17040    [LIVE, Attorney, General, Barr, hold, press, c...\n",
       "2472     [The, chain, flimsy, commitment, reminder, man...\n",
       "2034     [Every, Presidency, ha, dissenter, people, lea...\n",
       "12312    [Many, supermarket, remain, skeptical, deliver...\n",
       "3858     [The, New, York, Blood, Center, first, blood, ...\n",
       "11418    [Analysis, Trump, desire, short-term, good, ne...\n",
       "3019     [Investors, gird, choppy, market, race, White,...\n",
       "Name: clean, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 271
    }
   ],
   "source": [
    "df_raw['clean'].sample(20)"
   ]
  },
  {
   "source": [
    "### Remove any additional words like pronouns and other fillers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_words = ['The', \"the\", \"I\", \"he\", \"He\", \"She\", \"she\", \"her\", \"Her\", \"We\", \"we\", \"Me\", \"me\", \"Us\", \"us\", \"it\",\"It\", \"Them\", \"them\", \"They\", \"they\", \"There\", \"there\", \"that\", \"That\", \"This\", \"this\", \"You\", \"you\", \"who\", \"Who\", \"whom\" , \"Whom\", \"whose\", \"Whose\", \"what\", \"What\", \"which\", \"Which\", \"my\", \"My\", \"mine\",\"Mine\", \"your\", \"Your\", \"yours\", \"Yours\", \"his\", \"His\", \"hers\" \"Hers\", \"its\", \"Its\", \"It's\", \"it's\", \"Is\", \"is\" \"was\", \"Was\", \"In\", \"in\", \"across\", \"Across\", \"rather\", \"Rather\", \"roughly\", \"Roughly\", \"why\", \"Why\", \"where\", \"Where\", \"here\", \"Here\", \"A\", \"a\", \"Some\", \"some\", \"few\", \"Few\", \"None\", \"none\", \"more\", \"More\", \"Yes\", \"yes\", \"No\", \"no\", \"If\", \"if\", \"Let\", \"let\", \"Let's\", \"let's\", \"Came\", \"come\", \"go\", \"Go\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in pronoun_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14143    [Donald, Trump, ha, signed, legislation, new, ...\n",
       "5065                                   [Theodore, trouble]\n",
       "5372     [Coronavirus, update, Infections, rise, Univer...\n",
       "15058    [insertion, state, physician, office, ‚Äî, priva...\n",
       "1713     [General, Motors, ', Cruise, pandemic-proof, d...\n",
       "18722    [From, ice-cream, tin, food, wrap, made, seawe...\n",
       "5977     [coronavirus, lockdown, forced, many, people, ...\n",
       "14601    [half, year, since, last, Red, Ed, strike, end...\n",
       "17715    [Top, official, Brussels, believe, UK, populat...\n",
       "3278               [Worrying, people, behavior, n't, good]\n",
       "11126    [beauty, ha, floor-to-ceiling, window, take, S...\n",
       "10787    [latest, coronavirus, news, üëâNY, attorney-gene...\n",
       "16228    [Milagros, baby, Cristal, wa, born, premature,...\n",
       "13404    [Florida-sized, hunk, frozen, water, Antarctic...\n",
       "7014     [Los, Angeles, businessman, tip, led, governme...\n",
       "19064    [Perspective, massive, wave, eviction, coming....\n",
       "11398    [new, science, synthetic, biology, giving, u, ...\n",
       "13102    [President, give, shit, good, country, good, U...\n",
       "520      [Ireland, eurozone, biggest, loser, no-deal, B...\n",
       "51       [dozen, retail, pharmacy, chain, agreed, partn...\n",
       "Name: clean, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 281
    }
   ],
   "source": [
    "df_raw['clean'].sample(20)"
   ]
  },
  {
   "source": [
    "### Vectorize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean_vec'] = df_raw['clean'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                           Atlantic Daily Will decade new\n",
       "1        plenty going wrong Trump. four thing campaign ...\n",
       "2        Trump try steal election people surely protest...\n",
       "3        Trump campaign election-security operation key...\n",
       "4        Even Joe Biden win decisively next week coming...\n",
       "                               ...                        \n",
       "19995    General Mills make Cheerios cereal Yoplait yog...\n",
       "19996    From When viral panic subsides Fed rethink mor...\n",
       "19997           government want send money‚Äîbut soon arrive\n",
       "19998    frozen asparagus said one disappointed shopper...\n",
       "19999    coronavirus pandemic devastating U.S. restaura...\n",
       "Name: clean_vec, Length: 200000, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 283
    }
   ],
   "source": [
    "df_raw['clean_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = tfidf.fit_transform(df_raw['clean_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc_term_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    __  ___  ____  _____  ______________  _______________________  \\\n",
       "0  0.0  0.0   0.0    0.0             0.0                      0.0   \n",
       "1  0.0  0.0   0.0    0.0             0.0                      0.0   \n",
       "2  0.0  0.0   0.0    0.0             0.0                      0.0   \n",
       "3  0.0  0.0   0.0    0.0             0.0                      0.0   \n",
       "4  0.0  0.0   0.0    0.0             0.0                      0.0   \n",
       "\n",
       "   __________________________  __zz  _zz   aa  ...  Ô¨Çora  Ô¨Çow  Ô¨ÇuÔ¨Äy   Ô¨Çy  \\\n",
       "0                         0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0  0.0   \n",
       "1                         0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0  0.0   \n",
       "2                         0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0  0.0   \n",
       "3                         0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0  0.0   \n",
       "4                         0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0  0.0   \n",
       "\n",
       "   Ô¨Çygskam  Ô¨Çying  ùêÄùêØùêöùê¢ùê•ùêöùêõùê•ùêû  ùêîùê©ùêùùêöùê≠ùêûùê¨  ùòßùò∞ùò≥  ùò©ùò™ùòÆ  \n",
       "0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "1      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "2      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "3      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "4      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 57905 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>__</th>\n      <th>___</th>\n      <th>____</th>\n      <th>_____</th>\n      <th>______________</th>\n      <th>_______________________</th>\n      <th>__________________________</th>\n      <th>__zz</th>\n      <th>_zz</th>\n      <th>aa</th>\n      <th>...</th>\n      <th>Ô¨Çora</th>\n      <th>Ô¨Çow</th>\n      <th>Ô¨ÇuÔ¨Äy</th>\n      <th>Ô¨Çy</th>\n      <th>Ô¨Çygskam</th>\n      <th>Ô¨Çying</th>\n      <th>ùêÄùêØùêöùê¢ùê•ùêöùêõùê•ùêû</th>\n      <th>ùêîùê©ùêùùêöùê≠ùêûùê¨</th>\n      <th>ùòßùò∞ùò≥</th>\n      <th>ùò©ùò™ùòÆ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 57905 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['propellant', 'propelled', 'propeller', 'propelling', 'propels',\n",
       "       'propensity', 'proper', 'properly', 'properties', 'property',\n",
       "       ...\n",
       "       'raised', 'raiser', 'raisers', 'raises', 'raisin', 'raising', 'raison',\n",
       "       'raj', 'raja', 'rajabian'],\n",
       "      dtype='object', length=1000)"
      ]
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "source": [
    "df.columns[40000:41000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"doc_term_matrix_0.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        X = super().transform(raw_documents, copy=copy)\n",
    "        df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = super().fit_transform(raw_documents, y=y)\n",
    "        df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "        return df"
   ]
  }
 ]
}