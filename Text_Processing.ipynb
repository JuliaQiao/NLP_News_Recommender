{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_pickle('df_raw.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn to csv so we can store locally and store as database in sql\n",
    "df_raw.to_csv('/Users/juliaqiao/Documents/Metis/Proj_4_storage/df_raw.csv')"
   ]
  },
  {
   "source": [
    "### Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['user', 'date', 'outlinks', 'content', 'url'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex the columns for easier viewing\n",
    "cols = df_raw.columns.tolist()\n",
    "\n",
    "cols.insert(2, cols.pop(cols.index('url')))\n",
    "\n",
    "df_raw= df_raw.reindex(columns= cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary value of key: username\n",
    "# handle = [d[0].get('username') for d in df_raw.user if d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    Takes in tweet and performs initial text cleaning/preprocessing.\n",
    "    \"\"\"\n",
    "    #make sure doc is string\n",
    "    tweet=str(tweet)\n",
    "    #lowercase-- not changing anything to lowercase yet due to proper nouns being very important. want to use Spacy to detect later.\n",
    "    #tweet = tweet.lower()\n",
    "    #get rid of urls\n",
    "    rem_url=re.sub(r'http\\S+', '', tweet)\n",
    "    #gets rid of @ tags\n",
    "    rem_tag = re.sub('@\\S+', '', rem_url)\n",
    "    #gets rid of # in hashtag but keeps content of hashtag\n",
    "    rem_hashtag = re.sub('#', '', rem_tag)\n",
    "    #gets rid of numbers- holding off for now due to numbers being a great way to conserve space in twitter so they may be significant\n",
    "    #rem_num = re.sub('[0-9]+', '', rem_hashtag)\n",
    "    #gets rid of special characters\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', rem_hashtag)\n",
    "\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean']=df_raw['content'].map(lambda x:preprocess(x))"
   ]
  },
  {
   "source": [
    "### Tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TreebankWordTokenizer()\n",
    "df_raw['clean'] = df_raw['clean'].apply(tt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    " ### Lemmatize "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x]) # Lemmatize every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'is',\n",
       " 'devastating',\n",
       " 'U.S.',\n",
       " 'restaurant',\n",
       " 'chains.',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'why',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'company',\n",
       " 'should',\n",
       " 'see',\n",
       " 'the',\n",
       " 'light',\n",
       " 'of',\n",
       " 'day',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future.',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "#run example\n",
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    "### Remove Stop Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'devastating',\n",
       " 'U.S.',\n",
       " 'restaurant',\n",
       " 'chains.',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'company',\n",
       " 'see',\n",
       " 'light',\n",
       " 'day',\n",
       " 'near',\n",
       " 'future.',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "#run example\n",
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    "We did not use a stemmer. Lemmatizer was better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Remove any leftover punctuation:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', '/', ',', '%', '!', ':', \"'s\", \"'ve\", '\"', '*', '‚Äú', \"‚Äô\", \"`\", '?', '‚Äù', \"``\", \"‚Äò\"] \n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18215    [House, affordability, actually, getting, bett...\n",
       "1146     [Facts, truth, writes., The, truth, Breonna, T...\n",
       "615                                 [Just, doe, n't, mean]\n",
       "19380    [Like, surly, teen-ager, new, drama, series, E...\n",
       "9647     [Cut, eye, flatworm, ‚Äî, grow, back., Stick, ey...\n",
       "13355    [The, Svalbard, Satellite, Station, sits, insi...\n",
       "6303       [What, take, contortionist, Cirque, du, Soleil]\n",
       "16309    [I, 'm, private, online, tutor, wealthy, famil...\n",
       "2425     [', I, 'm, extremely, concerned, ', A, former,...\n",
       "17025    [Former, World, Bank, president, fault, Trump,...\n",
       "8999     [Advocates, believe, CBG, could, become, viabl...\n",
       "10764    [Opinion, The, problem, relying, precedent, pr...\n",
       "19511    [Brands, scrambling, amend, remove, rework, ad...\n",
       "12697    [Miss, today, AppleEvent, Here, breakdown, cat...\n",
       "16606    [Justice, Dept., scrutinizes, White, House-con...\n",
       "17072    [In, one, many, provocative, grammar, take, su...\n",
       "13376    [The, Parthenon, Marbles, British, Museum, sin...\n",
       "13094    [A, new, report, D.H.S., Inspector, General, r...\n",
       "10449    [Joe, Biden, suggested, Democrats, might, canc...\n",
       "7955     [You, might, wondering, Iowa, caucus, chaos, h...\n",
       "Name: clean, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "df_raw['clean'].sample(20)"
   ]
  },
  {
   "source": [
    "### Vectorize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean_vec'] = df_raw['clean'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                 The Atlantic Daily Will decade new 1850s\n",
       "1        There plenty going wrong Trump. Here four thin...\n",
       "2        If Trump try steal election people surely prot...\n",
       "3        The Trump campaign election-security operation...\n",
       "4        Even Joe Biden win decisively next week coming...\n",
       "                               ...                        \n",
       "19995    General Mills make Cheerios cereal Yoplait yog...\n",
       "19996    From When viral panic subsides Fed rethink mor...\n",
       "19997       The government want send money‚Äîbut soon arrive\n",
       "19998    They frozen asparagus said one disappointed sh...\n",
       "19999    The coronavirus pandemic devastating U.S. rest...\n",
       "Name: clean_vec, Length: 200000, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 240
    }
   ],
   "source": [
    "df_raw['clean_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = tfidf.fit_transform(df_raw['clean_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc_term_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    00  000  00000001  000003  0000047  000km  000kph  000m  000th  001  ...  \\\n",
       "0  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "1  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "2  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "3  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "4  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "\n",
       "   Ô¨Çora  Ô¨Çow  Ô¨ÇuÔ¨Äy   Ô¨Çy  Ô¨Çygskam  Ô¨Çying  ùêÄùêØùêöùê¢ùê•ùêöùêõùê•ùêû  ùêîùê©ùêùùêöùê≠ùêûùê¨  ùòßùò∞ùò≥  ùò©ùò™ùòÆ  \n",
       "0   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "1   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "2   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "3   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "4   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 59947 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>00000001</th>\n      <th>000003</th>\n      <th>0000047</th>\n      <th>000km</th>\n      <th>000kph</th>\n      <th>000m</th>\n      <th>000th</th>\n      <th>001</th>\n      <th>...</th>\n      <th>Ô¨Çora</th>\n      <th>Ô¨Çow</th>\n      <th>Ô¨ÇuÔ¨Äy</th>\n      <th>Ô¨Çy</th>\n      <th>Ô¨Çygskam</th>\n      <th>Ô¨Çying</th>\n      <th>ùêÄùêØùêöùê¢ùê•ùêöùêõùê•ùêû</th>\n      <th>ùêîùê©ùêùùêöùê≠ùêûùê¨</th>\n      <th>ùòßùò∞ùò≥</th>\n      <th>ùò©ùò™ùòÆ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 59947 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 258
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['abril', 'abroad', 'abrogation', 'abrupt', 'abruptly', 'abruzzi', 'abs',\n",
       "       'absence', 'absent', 'absentee', 'absenteeism', 'absentia', 'absinthe',\n",
       "       'absolute', 'absolutely', 'absolutes', 'absolution', 'absolutism',\n",
       "       'absolve', 'absolved', 'absolves', 'absorb', 'absorbable', 'absorbed',\n",
       "       'absorber', 'absorbing', 'absorbs', 'absorption', 'abstain',\n",
       "       'abstained', 'abstaining', 'abstains', 'abstemious', 'abstinence',\n",
       "       'abstract', 'abstracted', 'abstraction', 'abstruse', 'absurd',\n",
       "       'absurdism', 'absurdist', 'absurdity', 'absurdly', 'abt', 'abu',\n",
       "       'abubakar', 'abuela', 'abundance', 'abundant', 'abundantly', 'abus',\n",
       "       'abuse', 'abused', 'abuser', 'abusers', 'abuses', 'abusing', 'abusive',\n",
       "       'abusiveness', 'abus√≥', 'abut', 'abuzz', 'aby', 'abysmal', 'abyss',\n",
       "       'ac', 'aca', 'academia', 'academic', 'academically', 'academics',\n",
       "       'academy', 'acadia', 'acapulco', 'acb', 'acc', 'accede', 'accel',\n",
       "       'accelerant', 'accelerants', 'accelerate', 'accelerated', 'accelerates',\n",
       "       'accelerating', 'acceleration', 'accelerator', 'accelerators',\n",
       "       'accelerometer', 'accent', 'accents', 'accentuated', 'accentuates',\n",
       "       'accenture', 'accept', 'acceptability', 'acceptable', 'acceptance',\n",
       "       'accepted', 'accepters', 'accepting'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 255
    }
   ],
   "source": [
    "df.columns[2000:2100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        X = super().transform(raw_documents, copy=copy)\n",
    "        df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = super().fit_transform(raw_documents, y=y)\n",
    "        df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "        return df"
   ]
  }
 ]
}