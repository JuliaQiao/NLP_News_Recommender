{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_pickle('df_raw.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn to csv so we can store locally and store as database in sql\n",
    "df_raw.to_csv('/Users/juliaqiao/Documents/Metis/Proj_4_storage/df_raw.csv')"
   ]
  },
  {
   "source": [
    "### Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['user', 'date', 'outlinks', 'content', 'url'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex the columns for easier viewing\n",
    "cols = df_raw.columns.tolist()\n",
    "\n",
    "cols.insert(2, cols.pop(cols.index('url')))\n",
    "\n",
    "df_raw= df_raw.reindex(columns= cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary value of key: username\n",
    "# handle = [d[0].get('username') for d in df_raw.user if d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    Takes in tweet and performs initial text cleaning/preprocessing.\n",
    "    \"\"\"\n",
    "    #make sure doc is string\n",
    "    tweet=str(tweet)\n",
    "    #lowercase-- not changing anything to lowercase yet due to proper nouns being very important. want to use Spacy to detect later.\n",
    "    #tweet = tweet.lower()\n",
    "    #get rid of urls\n",
    "    rem_url=re.sub(r'http\\S+', '', tweet)\n",
    "    #gets rid of @ tags\n",
    "    rem_tag = re.sub('@\\S+', '', rem_url)\n",
    "    #gets rid of # in hashtag but keeps content of hashtag\n",
    "    rem_hashtag = re.sub('#', '', rem_tag)\n",
    "    #gets rid of numbers- holding off for now due to numbers being a great way to conserve space in twitter so they may be significant\n",
    "    #rem_num = re.sub('[0-9]+', '', rem_hashtag)\n",
    "    #gets rid of special characters\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', rem_hashtag)\n",
    "\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean']=df_raw['content'].map(lambda x:preprocess(x))"
   ]
  },
  {
   "source": [
    "### Tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TreebankWordTokenizer()\n",
    "df_raw['clean'] = df_raw['clean'].apply(tt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    " ### Lemmatize "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x]) # Lemmatize every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'is',\n",
       " 'devastating',\n",
       " 'U.S.',\n",
       " 'restaurant',\n",
       " 'chains.',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'why',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'company',\n",
       " 'should',\n",
       " 'see',\n",
       " 'the',\n",
       " 'light',\n",
       " 'of',\n",
       " 'day',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future.',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "#run example\n",
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    "### Remove Stop Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'devastating',\n",
       " 'U.S.',\n",
       " 'restaurant',\n",
       " 'chains.',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'company',\n",
       " 'see',\n",
       " 'light',\n",
       " 'day',\n",
       " 'near',\n",
       " 'future.',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "#run example\n",
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    "We did not use a stemmer. Lemmatizer was better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Remove any leftover punctuation:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', '/', ',', '%', '!', ':', \"'s\", \"'ve\", '\"', '*', '“', \"’\", \"`\", '?', '”', \"``\", \"‘\"] \n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18215    [House, affordability, actually, getting, bett...\n",
       "1146     [Facts, truth, writes., The, truth, Breonna, T...\n",
       "615                                 [Just, doe, n't, mean]\n",
       "19380    [Like, surly, teen-ager, new, drama, series, E...\n",
       "9647     [Cut, eye, flatworm, —, grow, back., Stick, ey...\n",
       "13355    [The, Svalbard, Satellite, Station, sits, insi...\n",
       "6303       [What, take, contortionist, Cirque, du, Soleil]\n",
       "16309    [I, 'm, private, online, tutor, wealthy, famil...\n",
       "2425     [', I, 'm, extremely, concerned, ', A, former,...\n",
       "17025    [Former, World, Bank, president, fault, Trump,...\n",
       "8999     [Advocates, believe, CBG, could, become, viabl...\n",
       "10764    [Opinion, The, problem, relying, precedent, pr...\n",
       "19511    [Brands, scrambling, amend, remove, rework, ad...\n",
       "12697    [Miss, today, AppleEvent, Here, breakdown, cat...\n",
       "16606    [Justice, Dept., scrutinizes, White, House-con...\n",
       "17072    [In, one, many, provocative, grammar, take, su...\n",
       "13376    [The, Parthenon, Marbles, British, Museum, sin...\n",
       "13094    [A, new, report, D.H.S., Inspector, General, r...\n",
       "10449    [Joe, Biden, suggested, Democrats, might, canc...\n",
       "7955     [You, might, wondering, Iowa, caucus, chaos, h...\n",
       "Name: clean, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "df_raw['clean'].sample(20)"
   ]
  },
  {
   "source": [
    "### Vectorize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean_vec'] = df_raw['clean'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                 The Atlantic Daily Will decade new 1850s\n",
       "1        There plenty going wrong Trump. Here four thin...\n",
       "2        If Trump try steal election people surely prot...\n",
       "3        The Trump campaign election-security operation...\n",
       "4        Even Joe Biden win decisively next week coming...\n",
       "                               ...                        \n",
       "19995    General Mills make Cheerios cereal Yoplait yog...\n",
       "19996    From When viral panic subsides Fed rethink mor...\n",
       "19997       The government want send money—but soon arrive\n",
       "19998    They frozen asparagus said one disappointed sh...\n",
       "19999    The coronavirus pandemic devastating U.S. rest...\n",
       "Name: clean_vec, Length: 200000, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 240
    }
   ],
   "source": [
    "df_raw['clean_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = tfidf.fit_transform(df_raw['clean_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc_term_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    00  000  00000001  000003  0000047  000km  000kph  000m  000th  001  ...  \\\n",
       "0  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "1  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "2  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "3  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "4  0.0  0.0       0.0     0.0      0.0    0.0     0.0   0.0    0.0  0.0  ...   \n",
       "\n",
       "   ﬂora  ﬂow  ﬂuﬀy   ﬂy  ﬂygskam  ﬂying  𝐀𝐯𝐚𝐢𝐥𝐚𝐛𝐥𝐞  𝐔𝐩𝐝𝐚𝐭𝐞𝐬  𝘧𝘰𝘳  𝘩𝘪𝘮  \n",
       "0   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "1   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "2   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "3   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "4   0.0  0.0   0.0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 59947 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>00000001</th>\n      <th>000003</th>\n      <th>0000047</th>\n      <th>000km</th>\n      <th>000kph</th>\n      <th>000m</th>\n      <th>000th</th>\n      <th>001</th>\n      <th>...</th>\n      <th>ﬂora</th>\n      <th>ﬂow</th>\n      <th>ﬂuﬀy</th>\n      <th>ﬂy</th>\n      <th>ﬂygskam</th>\n      <th>ﬂying</th>\n      <th>𝐀𝐯𝐚𝐢𝐥𝐚𝐛𝐥𝐞</th>\n      <th>𝐔𝐩𝐝𝐚𝐭𝐞𝐬</th>\n      <th>𝘧𝘰𝘳</th>\n      <th>𝘩𝘪𝘮</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 59947 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 258
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['abril', 'abroad', 'abrogation', 'abrupt', 'abruptly', 'abruzzi', 'abs',\n",
       "       'absence', 'absent', 'absentee', 'absenteeism', 'absentia', 'absinthe',\n",
       "       'absolute', 'absolutely', 'absolutes', 'absolution', 'absolutism',\n",
       "       'absolve', 'absolved', 'absolves', 'absorb', 'absorbable', 'absorbed',\n",
       "       'absorber', 'absorbing', 'absorbs', 'absorption', 'abstain',\n",
       "       'abstained', 'abstaining', 'abstains', 'abstemious', 'abstinence',\n",
       "       'abstract', 'abstracted', 'abstraction', 'abstruse', 'absurd',\n",
       "       'absurdism', 'absurdist', 'absurdity', 'absurdly', 'abt', 'abu',\n",
       "       'abubakar', 'abuela', 'abundance', 'abundant', 'abundantly', 'abus',\n",
       "       'abuse', 'abused', 'abuser', 'abusers', 'abuses', 'abusing', 'abusive',\n",
       "       'abusiveness', 'abusó', 'abut', 'abuzz', 'aby', 'abysmal', 'abyss',\n",
       "       'ac', 'aca', 'academia', 'academic', 'academically', 'academics',\n",
       "       'academy', 'acadia', 'acapulco', 'acb', 'acc', 'accede', 'accel',\n",
       "       'accelerant', 'accelerants', 'accelerate', 'accelerated', 'accelerates',\n",
       "       'accelerating', 'acceleration', 'accelerator', 'accelerators',\n",
       "       'accelerometer', 'accent', 'accents', 'accentuated', 'accentuates',\n",
       "       'accenture', 'accept', 'acceptability', 'acceptable', 'acceptance',\n",
       "       'accepted', 'accepters', 'accepting'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 255
    }
   ],
   "source": [
    "df.columns[2000:2100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        X = super().transform(raw_documents, copy=copy)\n",
    "        df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = super().fit_transform(raw_documents, y=y)\n",
    "        df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "        return df"
   ]
  }
 ]
}