{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# News Recommender: Natural Language Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*insert title page* with links"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## CLEANING AND VECTORIZATION\n",
    "In the first section of this notebook, we will be pre-processing, cleaning, lemmatizing, tagging, and vectorizing our webscraped tweets. The goal is to create an optimized document-term matrix for topic modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langdetect import detect\n",
    "import unicodedata"
   ]
  },
  {
   "source": [
    "### Standardize Encoding\n",
    "\n",
    "Read in CSV as dataframe, and standardize encoding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('df_raw.csv', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['user', 'date', 'url', 'outlinks', 'content'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex the columns for easier viewing\n",
    "cols = df_raw.columns.tolist()\n",
    "\n",
    "cols.insert(2, cols.pop(cols.index('url')))\n",
    "\n",
    "df_raw= df_raw.reindex(columns= cols)"
   ]
  },
  {
   "source": [
    "Decode our webscrapped tweets into ascii so we can remove emojis and foreign characters easily during our pre-processing steps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded unicode into ascii\n",
    "df_raw['clean'] = df_raw['content'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                user  \\\n",
       "0  {'username': 'TheAtlantic', 'displayname': 'Th...   \n",
       "1  {'username': 'TheAtlantic', 'displayname': 'Th...   \n",
       "2  {'username': 'TheAtlantic', 'displayname': 'Th...   \n",
       "3  {'username': 'TheAtlantic', 'displayname': 'Th...   \n",
       "4  {'username': 'TheAtlantic', 'displayname': 'Th...   \n",
       "\n",
       "                        date  \\\n",
       "0  2020-11-01T01:08:48+00:00   \n",
       "1  2020-11-01T00:38:45+00:00   \n",
       "2  2020-11-01T00:06:48+00:00   \n",
       "3  2020-10-31T23:34:45+00:00   \n",
       "4  2020-10-31T23:04:31+00:00   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://twitter.com/TheAtlantic/status/1322707...   \n",
       "1  https://twitter.com/TheAtlantic/status/1322699...   \n",
       "2  https://twitter.com/TheAtlantic/status/1322691...   \n",
       "3  https://twitter.com/TheAtlantic/status/1322683...   \n",
       "4  https://twitter.com/TheAtlantic/status/1322675...   \n",
       "\n",
       "                           outlinks  \\\n",
       "0  ['http://on.theatln.tc/YXH6gyR']   \n",
       "1  ['http://on.theatln.tc/l89Uzv7']   \n",
       "2  ['http://on.theatln.tc/ZGvkM7u']   \n",
       "3  ['http://on.theatln.tc/kypt5Zc']   \n",
       "4  ['http://on.theatln.tc/rNbarVc']   \n",
       "\n",
       "                                             content  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "3  The Trump campaign's “election-security operat...   \n",
       "4  Even if Joe Biden wins decisively next week, t...   \n",
       "\n",
       "                                               clean  \n",
       "0  The Atlantic Daily: Will this decade be the ne...  \n",
       "1  There's plenty that's going wrong for Trump. H...  \n",
       "2  If Trump tries to steal the election, people w...  \n",
       "3  The Trump campaign's election-security operati...  \n",
       "4  Even if Joe Biden wins decisively next week, t...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>url</th>\n      <th>outlinks</th>\n      <th>content</th>\n      <th>clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{'username': 'TheAtlantic', 'displayname': 'Th...</td>\n      <td>2020-11-01T01:08:48+00:00</td>\n      <td>https://twitter.com/TheAtlantic/status/1322707...</td>\n      <td>['http://on.theatln.tc/YXH6gyR']</td>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{'username': 'TheAtlantic', 'displayname': 'Th...</td>\n      <td>2020-11-01T00:38:45+00:00</td>\n      <td>https://twitter.com/TheAtlantic/status/1322699...</td>\n      <td>['http://on.theatln.tc/l89Uzv7']</td>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>{'username': 'TheAtlantic', 'displayname': 'Th...</td>\n      <td>2020-11-01T00:06:48+00:00</td>\n      <td>https://twitter.com/TheAtlantic/status/1322691...</td>\n      <td>['http://on.theatln.tc/ZGvkM7u']</td>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>If Trump tries to steal the election, people w...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>{'username': 'TheAtlantic', 'displayname': 'Th...</td>\n      <td>2020-10-31T23:34:45+00:00</td>\n      <td>https://twitter.com/TheAtlantic/status/1322683...</td>\n      <td>['http://on.theatln.tc/kypt5Zc']</td>\n      <td>The Trump campaign's “election-security operat...</td>\n      <td>The Trump campaign's election-security operati...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>{'username': 'TheAtlantic', 'displayname': 'Th...</td>\n      <td>2020-10-31T23:04:31+00:00</td>\n      <td>https://twitter.com/TheAtlantic/status/1322675...</td>\n      <td>['http://on.theatln.tc/rNbarVc']</td>\n      <td>Even if Joe Biden wins decisively next week, t...</td>\n      <td>Even if Joe Biden wins decisively next week, t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "source": [
    "### Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    Takes in tweet and performs initial text cleaning/preprocessing.\n",
    "    \"\"\"\n",
    "    #make sure doc is string\n",
    "    tweet=str(tweet)\n",
    "    #get rid of urls\n",
    "    rem_url=re.sub(r'http\\S+', '', tweet)\n",
    "    #gets rid of @ tags\n",
    "    rem_tag = re.sub('@\\S+', '', rem_url)\n",
    "    #gets rid of # in hashtag but keeps content of hashtag\n",
    "    rem_hashtag = re.sub('#', '', rem_tag)\n",
    "    #gets rid of special characters, numbers, etc.\n",
    "    clean_text = re.sub(r'[^A-Za-z\\s]','', rem_hashtag)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean']=df_raw['clean'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             content  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "\n",
       "                                               clean  \n",
       "0  The Atlantic Daily Will this decade be the new s   \n",
       "1  Theres plenty thats going wrong for Trump Here...  \n",
       "2  If Trump tries to steal the election people wi...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>The Atlantic Daily Will this decade be the new s</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>Theres plenty thats going wrong for Trump Here...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>If Trump tries to steal the election people wi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "df_raw[['content', 'clean']].head(3)"
   ]
  },
  {
   "source": [
    "### Standardize Language\n",
    "\n",
    "Let's remove any foreign language tweets to make sure we're only focusing on English. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_only(x):\n",
    "    \"\"\"\n",
    "    Take tweet, detect language, and only return English tweets, coding foreign language tweets as NaNs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if detect(x) == 'en':\n",
    "            return x\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 12min 4s, sys: 6.02 s, total: 12min 10s\nWall time: 12min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#  Remove any non english tweets\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: english_only(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1251"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "df_raw.clean.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop non-English tweets\n",
    "df_raw = df_raw[df_raw.clean.notnull()]"
   ]
  },
  {
   "source": [
    "### Tagging and Lemmatizing\n",
    "\n",
    "We only want the nouns (and proper nouns) in each tweet for topic modeling, as they are the essence of article subjects. Let's tag the nouns and return the lemmatized versions of tehm in a single step.\n",
    "\n",
    "Because we also have so much data, we will incoporate the NLP pipeline in order to shorten the processing time. Tips on how to do this were found here: https://towardsdatascience.com/turbo-charge-your-spacy-nlp-pipeline-551435b664ad"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[ 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_lemmatize_pipe(doc):\n",
    "    \"\"\"\n",
    "    Takes in tweet and returns only the lemmatized version of nouns (including proper nouns).\n",
    "    \"\"\"\n",
    "    lemma_list = [token.lemma_ for token in doc\n",
    "                  if token.pos_ == \"NOUN\" or token.pos_ ==\"PROPN\"] \n",
    "    return lemma_list\n",
    "\n",
    "#create a pipeline in order to shorten processing time\n",
    "def preprocess_pipe(texts):\n",
    "    \"\"\"\n",
    "    Inputs noun_lemmative_pipe function into NLP pipeline for faster processing.\n",
    "    \"\"\"\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=50):\n",
    "        preproc_pipe.append(noun_lemmatize_pipe(doc))\n",
    "    return preproc_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2min 22s, sys: 670 ms, total: 2min 23s\nWall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#apply function and create a new column to house the outputs\n",
    "df_raw['clean_lemmatized'] = preprocess_pipe(df_raw['clean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             content  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  The Atlantic Daily Will this decade be the new s    \n",
       "1  Theres plenty thats going wrong for Trump Here...   \n",
       "2  If Trump tries to steal the election people wi...   \n",
       "\n",
       "                                    clean_lemmatized  \n",
       "0                                        [decade, s]  \n",
       "1  [plenty, Trump, thing, campaign, gap, Joe, Bid...  \n",
       "2   [Trump, election, people, coup, strategy, write]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>clean</th>\n      <th>clean_lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>The Atlantic Daily Will this decade be the new s</td>\n      <td>[decade, s]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>Theres plenty thats going wrong for Trump Here...</td>\n      <td>[plenty, Trump, thing, campaign, gap, Joe, Bid...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>If Trump tries to steal the election people wi...</td>\n      <td>[Trump, election, people, coup, strategy, write]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "df_raw[['content', 'clean', 'clean_lemmatized']].head(3)"
   ]
  },
  {
   "source": [
    "We've successfully filtered out the nouns and proper nouns, lemmitized them, while making sure our function runs on optimized time!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Remove Additional Words\n",
    "\n",
    "Let's filter out any additional words that may appear in the tweets but aren't related to article subjects, like the names of the publications and common headline section titles."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "removal_words= [\"Times\", \"Wall\", \"Street\", \"Journal\", \"New\", \"Yorker\", \"York\", \"Medium\", \"Wired\", \"Financial\", \"Washington\", \"Post\", \"Business\", \"Insider\", \"Economist\", \"The\", \"Atlantic\", \"Daily\", \"Weekly\", \"SPONSORED\", \"Sponsored\", \"BREAKING\", \"Breaking\", \"NEWS\", \"News\" ]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean_lemmatized'] = df_raw['clean_lemmatized'].apply(lambda x: [word for word in x if word not in removal_words])"
   ]
  },
  {
   "source": [
    "### Vectorize\n",
    "Now let's rejoin our twice cleaned, lemmatized list of nouns and pronouns!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean_final'] = df_raw['clean_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             content  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  The Atlantic Daily Will this decade be the new s    \n",
       "1  Theres plenty thats going wrong for Trump Here...   \n",
       "2  If Trump tries to steal the election people wi...   \n",
       "\n",
       "                                    clean_lemmatized  \\\n",
       "0                                        [decade, s]   \n",
       "1  [plenty, Trump, thing, campaign, gap, Joe, Bid...   \n",
       "2   [Trump, election, people, coup, strategy, write]   \n",
       "\n",
       "                                        clean_final  \n",
       "0                                          decade s  \n",
       "1  plenty Trump thing campaign gap Joe Biden report  \n",
       "2         Trump election people coup strategy write  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>clean</th>\n      <th>clean_lemmatized</th>\n      <th>clean_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>The Atlantic Daily Will this decade be the new s</td>\n      <td>[decade, s]</td>\n      <td>decade s</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>Theres plenty thats going wrong for Trump Here...</td>\n      <td>[plenty, Trump, thing, campaign, gap, Joe, Bid...</td>\n      <td>plenty Trump thing campaign gap Joe Biden report</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>If Trump tries to steal the election people wi...</td>\n      <td>[Trump, election, people, coup, strategy, write]</td>\n      <td>Trump election people coup strategy write</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "df_raw[['content', 'clean', 'clean_lemmatized', 'clean_final']].head(3)"
   ]
  },
  {
   "source": [
    "We will be using TFID Vectorizer as opposed to Count Vectorizer, so we can give equal weight to rare words. Because our focus is on nouns and pronouns, rare words are likely to be just as, if not more impactful, as words that are frequently used. \n",
    "\n",
    "We'll also be using some of the built in parameters in the TFID Vectorizer as last checks before we output our doc-term matrix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer and set parameters in order to standardize everything to lowercase, remove any stop words, and remove any word that appears below 0.005%, about 10 times.\n",
    "tfidf = TfidfVectorizer(lowercase = True, stop_words= 'english', min_df = 0.00005)\n",
    "#fit on fully cleaned dataframe column\n",
    "doc_term_matrix = tfidf.fit_transform(df_raw['clean_final'])\n",
    "#turn matrix into a dataframe with words as columns\n",
    "matrix_df = pd.DataFrame(doc_term_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        aaron   ab  abandonment  abbey  abbott  abby  abc  abe  abenomics  \\\n",
       "0         0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "1         0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "2         0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "3         0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "4         0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "...       ...  ...          ...    ...     ...   ...  ...  ...        ...   \n",
       "198744    0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "198745    0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "198746    0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "198747    0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "198748    0.0  0.0          0.0    0.0     0.0   0.0  0.0  0.0        0.0   \n",
       "\n",
       "        aberration  ...  zoo  zoological  zoom  zooms  zoos  zoox  zora  \\\n",
       "0              0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "1              0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "2              0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "3              0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "4              0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "...            ...  ...  ...         ...   ...    ...   ...   ...   ...   \n",
       "198744         0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "198745         0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "198746         0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "198747         0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "198748         0.0  ...  0.0         0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "\n",
       "        zuckerberg  zuckerbergs  zuzana  \n",
       "0              0.0          0.0     0.0  \n",
       "1              0.0          0.0     0.0  \n",
       "2              0.0          0.0     0.0  \n",
       "3              0.0          0.0     0.0  \n",
       "4              0.0          0.0     0.0  \n",
       "...            ...          ...     ...  \n",
       "198744         0.0          0.0     0.0  \n",
       "198745         0.0          0.0     0.0  \n",
       "198746         0.0          0.0     0.0  \n",
       "198747         0.0          0.0     0.0  \n",
       "198748         0.0          0.0     0.0  \n",
       "\n",
       "[198749 rows x 10671 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaron</th>\n      <th>ab</th>\n      <th>abandonment</th>\n      <th>abbey</th>\n      <th>abbott</th>\n      <th>abby</th>\n      <th>abc</th>\n      <th>abe</th>\n      <th>abenomics</th>\n      <th>aberration</th>\n      <th>...</th>\n      <th>zoo</th>\n      <th>zoological</th>\n      <th>zoom</th>\n      <th>zooms</th>\n      <th>zoos</th>\n      <th>zoox</th>\n      <th>zora</th>\n      <th>zuckerberg</th>\n      <th>zuckerbergs</th>\n      <th>zuzana</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>198744</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>198745</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>198746</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>198747</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>198748</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>198749 rows × 10671 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "matrix_df"
   ]
  },
  {
   "source": [
    "Our doc-term matrix looks great! We have fully cleaned out all special characters and foreign language. We have extracted the nouns and pronouns in their lemmatized forms. Now we can topic model!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TOPIC MODELING\n",
    "In the second section of this notebook, we will be topic modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "source": [
    "### LSA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00313351, 0.00424593, 0.00357008, 0.00321061, 0.00294962,\n",
       "       0.00292731, 0.00283308, 0.00280643, 0.00272183, 0.00256971])"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "lsa = TruncatedSVD(10)\n",
    "doc_topic = lsa.fit_transform(doc_term_matrix)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              aaron   ab  abandonment  abbey  abbott  abby    abc    abe  \\\n",
       "component_1   0.000  0.0          0.0    0.0   0.001   0.0  0.001  0.002   \n",
       "component_2   0.000 -0.0          0.0    0.0   0.000  -0.0  0.001  0.001   \n",
       "component_3   0.000  0.0          0.0    0.0  -0.001   0.0  0.000  0.000   \n",
       "component_4  -0.000 -0.0         -0.0   -0.0   0.000   0.0 -0.000 -0.001   \n",
       "component_5   0.000  0.0          0.0   -0.0  -0.001   0.0 -0.000  0.000   \n",
       "component_6   0.000 -0.0          0.0   -0.0   0.001   0.0 -0.000 -0.000   \n",
       "component_7   0.000  0.0          0.0   -0.0   0.000   0.0 -0.000  0.001   \n",
       "component_8   0.000 -0.0          0.0   -0.0   0.000   0.0 -0.000  0.001   \n",
       "component_9   0.001 -0.0         -0.0    0.0   0.000  -0.0 -0.000  0.001   \n",
       "component_10  0.000  0.0          0.0    0.0   0.000   0.0 -0.001  0.001   \n",
       "\n",
       "              abenomics  aberration  ...    zoo  zoological   zoom  zooms  \\\n",
       "component_1         0.0         0.0  ...  0.001         0.0  0.006    0.0   \n",
       "component_2         0.0         0.0  ... -0.001        -0.0 -0.002   -0.0   \n",
       "component_3         0.0         0.0  ...  0.000         0.0  0.006    0.0   \n",
       "component_4        -0.0         0.0  ... -0.000        -0.0 -0.001   -0.0   \n",
       "component_5         0.0         0.0  ... -0.000        -0.0  0.002    0.0   \n",
       "component_6         0.0         0.0  ... -0.000        -0.0  0.000    0.0   \n",
       "component_7        -0.0        -0.0  ... -0.000        -0.0 -0.001    0.0   \n",
       "component_8        -0.0         0.0  ... -0.000        -0.0 -0.003   -0.0   \n",
       "component_9         0.0        -0.0  ...  0.000         0.0 -0.001    0.0   \n",
       "component_10       -0.0         0.0  ...  0.001         0.0  0.004    0.0   \n",
       "\n",
       "               zoos  zoox   zora  zuckerberg  zuckerbergs  zuzana  \n",
       "component_1   0.000   0.0  0.000       0.005        0.001     0.0  \n",
       "component_2  -0.000  -0.0 -0.000       0.001        0.000     0.0  \n",
       "component_3  -0.000   0.0  0.000       0.007        0.000     0.0  \n",
       "component_4  -0.000  -0.0 -0.000      -0.004       -0.001     0.0  \n",
       "component_5   0.000  -0.0 -0.000      -0.001        0.000     0.0  \n",
       "component_6   0.001  -0.0  0.000      -0.001       -0.000     0.0  \n",
       "component_7  -0.000   0.0  0.000       0.002        0.000    -0.0  \n",
       "component_8  -0.000  -0.0  0.000       0.001       -0.000    -0.0  \n",
       "component_9  -0.000  -0.0 -0.000       0.001       -0.000    -0.0  \n",
       "component_10 -0.000   0.0  0.001       0.005        0.000     0.0  \n",
       "\n",
       "[10 rows x 10671 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaron</th>\n      <th>ab</th>\n      <th>abandonment</th>\n      <th>abbey</th>\n      <th>abbott</th>\n      <th>abby</th>\n      <th>abc</th>\n      <th>abe</th>\n      <th>abenomics</th>\n      <th>aberration</th>\n      <th>...</th>\n      <th>zoo</th>\n      <th>zoological</th>\n      <th>zoom</th>\n      <th>zooms</th>\n      <th>zoos</th>\n      <th>zoox</th>\n      <th>zora</th>\n      <th>zuckerberg</th>\n      <th>zuckerbergs</th>\n      <th>zuzana</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>component_1</th>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.002</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.006</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.005</td>\n      <td>0.001</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>component_2</th>\n      <td>0.000</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>-0.0</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.001</td>\n      <td>-0.0</td>\n      <td>-0.002</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>component_3</th>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.006</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>component_4</th>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>-0.001</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>-0.001</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-0.004</td>\n      <td>-0.001</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>component_5</th>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>-0.001</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>0.002</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-0.001</td>\n      <td>0.000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>component_6</th>\n      <td>0.000</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>-0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>-0.0</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>-0.000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>component_7</th>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.001</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>...</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>-0.001</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.000</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>component_8</th>\n      <td>0.000</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.001</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>-0.003</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>component_9</th>\n      <td>0.001</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>-0.001</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n      <td>-0.000</td>\n      <td>0.001</td>\n      <td>-0.000</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>component_10</th>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>-0.001</td>\n      <td>0.001</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.004</td>\n      <td>0.0</td>\n      <td>-0.000</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.005</td>\n      <td>0.000</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 10671 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\", \"component_4\", \"component_5\", \n",
    "             \"component_6\", \"component_7\", \"component_8\", \"component_9\", \"component_10\"],\n",
    "             columns = tfidf.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\ncoronavirus, people, trump, pandemic, president, year, world, time, case, opinion\n\nTopic  1\ntrump, president, opinion, donald, biden, joe, trumps, election, campaign, house\n\nTopic  2\npeople, year, time, life, world, way, thing, company, story, home\n\nTopic  3\npeople, trump, coronavirus, president, opinion, case, donald, white, house, virus\n\nTopic  4\npandemic, opinion, time, way, world, life, thing, trumps, woman, write\n\nTopic  5\ntime, day, case, coronavirus, death, state, briefing, page, opinion, morning\n\nTopic  6\npage, look, world, edition, uk, thing, today, friday, monday, thursday\n\nTopic  7\nworld, case, week, coronavirus, story, life, country, way, woman, outbreak\n\nTopic  8\nopinion, year, day, coronavirus, china, case, america, state, hong, page\n\nTopic  9\nway, day, story, life, thing, week, election, morning, hong, briefing\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### NMF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(10)\n",
    "doc_topic = nmf.fit_transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              aaron     ab  abandonment  abbey  abbott   abby    abc    abe  \\\n",
       "component_1   0.001  0.000        0.001  0.000   0.012  0.000  0.000  0.004   \n",
       "component_2   0.001  0.000        0.000  0.000   0.003  0.000  0.006  0.004   \n",
       "component_3   0.002  0.000        0.000  0.003   0.000  0.000  0.003  0.004   \n",
       "component_4   0.000  0.000        0.000  0.000   0.000  0.001  0.000  0.000   \n",
       "component_5   0.000  0.001        0.000  0.000   0.000  0.000  0.000  0.000   \n",
       "component_6   0.001  0.000        0.001  0.000   0.000  0.000  0.001  0.003   \n",
       "component_7   0.000  0.000        0.000  0.000   0.000  0.000  0.000  0.001   \n",
       "component_8   0.000  0.000        0.001  0.000   0.000  0.001  0.000  0.007   \n",
       "component_9   0.004  0.000        0.000  0.000   0.000  0.000  0.000  0.005   \n",
       "component_10  0.001  0.000        0.000  0.001   0.001  0.000  0.000  0.002   \n",
       "\n",
       "              abenomics  aberration  ...    zoo  zoological   zoom  zooms  \\\n",
       "component_1       0.000       0.000  ...  0.006       0.000  0.000  0.001   \n",
       "component_2       0.000       0.001  ...  0.000       0.000  0.000  0.000   \n",
       "component_3       0.001       0.000  ...  0.003       0.001  0.014  0.000   \n",
       "component_4       0.000       0.001  ...  0.003       0.000  0.026  0.000   \n",
       "component_5       0.000       0.000  ...  0.001       0.000  0.028  0.001   \n",
       "component_6       0.001       0.000  ...  0.000       0.000  0.014  0.000   \n",
       "component_7       0.000       0.000  ...  0.000       0.000  0.001  0.000   \n",
       "component_8       0.000       0.000  ...  0.000       0.000  0.002  0.000   \n",
       "component_9       0.003       0.000  ...  0.000       0.000  0.004  0.000   \n",
       "component_10      0.000       0.000  ...  0.002       0.000  0.018  0.000   \n",
       "\n",
       "               zoos   zoox   zora  zuckerberg  zuckerbergs  zuzana  \n",
       "component_1   0.003  0.000  0.000       0.000        0.000   0.000  \n",
       "component_2   0.001  0.000  0.000       0.008        0.002   0.000  \n",
       "component_3   0.000  0.001  0.001       0.019        0.002   0.001  \n",
       "component_4   0.000  0.001  0.001       0.016        0.000   0.001  \n",
       "component_5   0.001  0.000  0.000       0.002        0.001   0.000  \n",
       "component_6   0.003  0.000  0.000       0.007        0.002   0.001  \n",
       "component_7   0.000  0.000  0.001       0.008        0.001   0.000  \n",
       "component_8   0.000  0.000  0.000       0.018        0.001   0.000  \n",
       "component_9   0.000  0.000  0.000       0.005        0.000   0.000  \n",
       "component_10  0.000  0.000  0.003       0.031        0.001   0.001  \n",
       "\n",
       "[10 rows x 10671 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaron</th>\n      <th>ab</th>\n      <th>abandonment</th>\n      <th>abbey</th>\n      <th>abbott</th>\n      <th>abby</th>\n      <th>abc</th>\n      <th>abe</th>\n      <th>abenomics</th>\n      <th>aberration</th>\n      <th>...</th>\n      <th>zoo</th>\n      <th>zoological</th>\n      <th>zoom</th>\n      <th>zooms</th>\n      <th>zoos</th>\n      <th>zoox</th>\n      <th>zora</th>\n      <th>zuckerberg</th>\n      <th>zuckerbergs</th>\n      <th>zuzana</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>component_1</th>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.012</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.004</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.006</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>component_2</th>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.006</td>\n      <td>0.004</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.008</td>\n      <td>0.002</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>component_3</th>\n      <td>0.002</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.004</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.003</td>\n      <td>0.001</td>\n      <td>0.014</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.019</td>\n      <td>0.002</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>component_4</th>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>...</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.026</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.016</td>\n      <td>0.000</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>component_5</th>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.028</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>component_6</th>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.014</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.002</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>component_7</th>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.008</td>\n      <td>0.001</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>component_8</th>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.018</td>\n      <td>0.001</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>component_9</th>\n      <td>0.004</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.005</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.004</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.005</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>component_10</th>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0.002</td>\n      <td>0.000</td>\n      <td>0.018</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.031</td>\n      <td>0.001</td>\n      <td>0.001</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 10671 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(nmf.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\", \"component_4\", \"component_5\", \n",
    "             \"component_6\", \"component_7\", \"component_8\", \"component_9\", \"component_10\"],\n",
    "             columns = tfidf.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\ncoronavirus, case, death, state, outbreak, country, vaccine, crisis, infection, number\n\nTopic  1\ntrump, president, donald, biden, joe, trumps, election, analysis, house, campaign\n\nTopic  2\nyear, company, market, stock, month, business, report, school, government, tech\n\nTopic  3\npeople, life, home, covid, way, thing, city, police, virus, work\n\nTopic  4\npandemic, way, company, business, life, economy, job, worker, money, covid\n\nTopic  5\ntime, thing, life, way, write, woman, money, home, child, crisis\n\nTopic  6\npage, look, edition, uk, friday, thursday, monday, today, tuesday, wednesday\n\nTopic  7\nworld, country, way, china, war, today, week, company, climate, change\n\nTopic  8\nopinion, trumps, america, republicans, trump, way, court, crisis, republican, election\n\nTopic  9\nday, story, briefing, morning, election, week, hong, kong, end, europe\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf, tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "source": [
    "#### Next steps:\n",
    "NMF works better than LDA\n",
    "\n",
    "add words below to removal_list before vectorizing: way, page, edition, morning, monday, tuesday, weds, etc. , thing, briefing, day, story, year, life, write, wrote"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}