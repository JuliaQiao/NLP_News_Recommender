{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[ 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_pickle('df_raw.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex the columns for easier viewing\n",
    "cols = df_raw.columns.tolist()\n",
    "\n",
    "cols.insert(2, cols.pop(cols.index('url')))\n",
    "\n",
    "df_raw= df_raw.reindex(columns= cols)"
   ]
  },
  {
   "source": [
    "### Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    Takes in tweet and performs initial text cleaning/preprocessing.\n",
    "    \"\"\"\n",
    "    #make sure doc is string\n",
    "    tweet=str(tweet)\n",
    "    #lowercase-- not changing anything to lowercase yet due to proper nouns being very important. want to use Spacy to detect later.\n",
    "    #tweet = tweet.lower()\n",
    "    #get rid of urls\n",
    "    rem_url=re.sub(r'http\\S+', '', tweet)\n",
    "    #gets rid of @ tags\n",
    "    rem_tag = re.sub('@\\S+', '', rem_url)\n",
    "    #gets rid of # in hashtag but keeps content of hashtag\n",
    "    rem_hashtag = re.sub('#', '', rem_tag)\n",
    "    #gets rid of numbers\n",
    "    rem_num = re.sub('[0-9]+', '', rem_hashtag)\n",
    "    #gets rid of special characters\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', rem_hashtag)\n",
    "\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean']=df_raw['content'].map(lambda x:preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The coronavirus pandemic is devastating U.S. restaurant chains. But  explains why financially stronger companies should see the light of day in the near future. WSJWhatsNow  '"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    "### Tagging and Lemmatizing\n",
    "\n",
    "We only want the nouns (and proper nouns) in each tweet for topic modeling, as they are the essence of article subjects. Let's tag the nouns and return the lemmatized versions of tehm in a single step.\n",
    "\n",
    "Because we also have so much data, we will incoporate the NLP pipeline in order to shorten the processing time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_lemmatize_pipe(doc):\n",
    "    \"\"\"\n",
    "    Takes in tweet and returns only the lemmatized version of nouns (including proper nouns).\n",
    "    \"\"\"\n",
    "    lemma_list = [token.lemma_ for token in doc\n",
    "                  if token.pos_ == \"NOUN\" or token.pos_ ==\"PROPN\"] \n",
    "    return lemma_list\n",
    "\n",
    "#create a pipeline in order to shorten processing time\n",
    "def preprocess_pipe(texts):\n",
    "    \"\"\"\n",
    "    Inputs noun_lemmative_pipe function into NLP pipeline for faster processing.\n",
    "    \"\"\"\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=20):\n",
    "        preproc_pipe.append(noun_lemmatize_pipe(doc))\n",
    "    return preproc_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_raw.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2min 47s, sys: 738 ms, total: 2min 48s\nWall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "#let's time our function\n",
    "%%time\n",
    "#apply function and create a new column to house the outputs\n",
    "df_raw['clean_lemmatized'] = preprocess_pipe(df_raw['clean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             content  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "\n",
       "                                    clean_lemmatized  \n",
       "0                          [Atlantic, Daily, decade]  \n",
       "1  [plenty, Trump, thing, campaign, gap, Joe, Bid...  \n",
       "2   [Trump, election, people, coup, strategy, write]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>clean</th>\n      <th>clean_lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>[Atlantic, Daily, decade]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>[plenty, Trump, thing, campaign, gap, Joe, Bid...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>[Trump, election, people, coup, strategy, write]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "df_raw[['content', 'clean', 'clean_lemmatized']].head(3)"
   ]
  },
  {
   "source": [
    "We've successfully filtered out the nouns and proper nouns, lemmitized them, while making sure our function runs on optimized time!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Remove Additional Words\n",
    "\n",
    "Let's filter out any additional words that may appear in the tweets but aren't related to article subjects, like the names of the publications and common headline section titles."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "removal_words= [\"Times\", \"Wall\", \"Street\", \"Journal\", \"New\", \"Yorker\", \"York\", \"Medium\", \"Wired\", \"Financial\", \"Washington\", \"Post\", \"Business\", \"Insider\", \"Economist\", \"The\", \"Atlantic\", \"Daily\", \"Weekly\" ]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 87,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean_lemmatized'] = df_raw['clean_lemmatized'].apply(lambda x: [word for word in x if word not in removal_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             content  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "\n",
       "                                    clean_lemmatized  \n",
       "0                                           [decade]  \n",
       "1  [plenty, Trump, thing, campaign, gap, Joe, Bid...  \n",
       "2   [Trump, election, people, coup, strategy, write]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>clean</th>\n      <th>clean_lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>[decade]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>[plenty, Trump, thing, campaign, gap, Joe, Bid...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>[Trump, election, people, coup, strategy, write]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "df_raw[['content', 'clean', 'clean_lemmatized']].head(3)"
   ]
  },
  {
   "source": [
    "### Vectorize\n",
    "Now let's rejoin our twice cleaned, lemmatized list of nouns and pronouns!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean_final'] = df_raw['clean_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             content  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  The Atlantic Daily: Will this decade be the ne...   \n",
       "1  There's plenty that's going wrong for Trump. H...   \n",
       "2  If Trump tries to steal the election, people w...   \n",
       "\n",
       "                                    clean_lemmatized  \\\n",
       "0                                           [decade]   \n",
       "1  [plenty, Trump, thing, campaign, gap, Joe, Bid...   \n",
       "2   [Trump, election, people, coup, strategy, write]   \n",
       "\n",
       "                                        clean_final  \n",
       "0                                            decade  \n",
       "1  plenty Trump thing campaign gap Joe Biden report  \n",
       "2         Trump election people coup strategy write  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>clean</th>\n      <th>clean_lemmatized</th>\n      <th>clean_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>The Atlantic Daily: Will this decade be the ne...</td>\n      <td>[decade]</td>\n      <td>decade</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>There's plenty that's going wrong for Trump. H...</td>\n      <td>[plenty, Trump, thing, campaign, gap, Joe, Bid...</td>\n      <td>plenty Trump thing campaign gap Joe Biden report</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>If Trump tries to steal the election, people w...</td>\n      <td>[Trump, election, people, coup, strategy, write]</td>\n      <td>Trump election people coup strategy write</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "df_raw[['content', 'clean', 'clean_lemmatized', 'clean_final']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "#fit on fully cleaned dataframe column\n",
    "doc_term_matrix = tfidf.fit_transform(df_raw['clean_final'])\n",
    "#turn matrix into a dataframe with words as columns\n",
    "matrix_df = pd.DataFrame(doc_term_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         00  000   03   07   08   09   0s   10  100  100bn  ...  Ô¨Çoor  Ô¨Çora  \\\n",
       "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...    ...  ...   ...   ...   \n",
       "199995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "199996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "199997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "199998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "199999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  ...   0.0   0.0   \n",
       "\n",
       "        Ô¨Çow  Ô¨ÇuÔ¨Äy  Ô¨Çygskam  Ô¨Çying  ùêÄùêØùêöùê¢ùê•ùêöùêõùê•ùêû  ùêîùê©ùêùùêöùê≠ùêûùê¨  ùòßùò∞ùò≥  ùò©ùò™ùòÆ  \n",
       "0       0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "1       0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "2       0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "3       0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "4       0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "...     ...   ...      ...    ...        ...      ...  ...  ...  \n",
       "199995  0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "199996  0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "199997  0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "199998  0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "199999  0.0   0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "\n",
       "[200000 rows x 42145 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>03</th>\n      <th>07</th>\n      <th>08</th>\n      <th>09</th>\n      <th>0s</th>\n      <th>10</th>\n      <th>100</th>\n      <th>100bn</th>\n      <th>...</th>\n      <th>Ô¨Çoor</th>\n      <th>Ô¨Çora</th>\n      <th>Ô¨Çow</th>\n      <th>Ô¨ÇuÔ¨Äy</th>\n      <th>Ô¨Çygskam</th>\n      <th>Ô¨Çying</th>\n      <th>ùêÄùêØùêöùê¢ùê•ùêöùêõùê•ùêû</th>\n      <th>ùêîùê©ùêùùêöùê≠ùêûùê¨</th>\n      <th>ùòßùò∞ùò≥</th>\n      <th>ùò©ùò™ùòÆ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>199995</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>199996</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>199997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>199998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>199999</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>200000 rows √ó 42145 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['000', '03', '07', '08', '09', '0s', '10', '100', '100bn', '104bn',\n",
       "       ...\n",
       "       '356bn', '35bn', '36', '360bn', '360i', '369bn', '36bn', '37', '370bn',\n",
       "       '37b'],\n",
       "      dtype='object', length=199)"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "matrix_df.columns[1:200]"
   ]
  },
  {
   "source": [
    "### Next Steps\n",
    "This doc-term matrix with Spacy tagging and lemmitization is much better than only using NLTK. We've cut down by 1000 terms/columns! \n",
    "\n",
    "However, we still need to clean out the columns that have special characters and numbers we weren't able to catch the first time around"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}