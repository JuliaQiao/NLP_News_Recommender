{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import emoji\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_pickle('df_raw.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn to csv so we can store locally and store as database in sql\n",
    "df_raw.to_csv('/Users/juliaqiao/Documents/Metis/Proj_4_storage/df_raw.csv')"
   ]
  },
  {
   "source": [
    "### Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['user', 'date', 'outlinks', 'content', 'url'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex the columns for easier viewing\n",
    "cols = df_raw.columns.tolist()\n",
    "\n",
    "cols.insert(2, cols.pop(cols.index('url')))\n",
    "\n",
    "df_raw= df_raw.reindex(columns= cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary value of key: username\n",
    "# handle = [d[0].get('username') for d in df_raw.user if d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    Takes in tweet and performs initial text cleaning/preprocessing.\n",
    "    \"\"\"\n",
    "    #make sure doc is string\n",
    "    tweet=str(tweet)\n",
    "    #lowercase-- not changing anything to lowercase yet due to proper nouns being very important. want to use Spacy to detect later.\n",
    "    #tweet = tweet.lower()\n",
    "    #get rid of urls\n",
    "    rem_url=re.sub(r'http\\S+', '', tweet)\n",
    "    #gets rid of @ tags\n",
    "    rem_tag = re.sub('@\\S+', '', rem_url)\n",
    "    #gets rid of # in hashtag but keeps content of hashtag\n",
    "    rem_hashtag = re.sub('#', '', rem_tag)\n",
    "    #gets rid of numbers\n",
    "    rem_num = re.sub('[0-9]+', '', rem_hashtag)\n",
    "    #gets rid of emojis\n",
    "    rem_emoji = re.sub(r'[^\\w\\s,]', '', rem_num)\n",
    "    #gets rid of special characters\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', rem_emoji)\n",
    "\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean']=df_raw['content'].map(lambda x:preprocess(x))"
   ]
  },
  {
   "source": [
    "### Tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TreebankWordTokenizer()\n",
    "df_raw['clean'] = df_raw['clean'].apply(tt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'is',\n",
       " 'devastating',\n",
       " 'US',\n",
       " 'restaurant',\n",
       " 'chains',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'why',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'companies',\n",
       " 'should',\n",
       " 'see',\n",
       " 'the',\n",
       " 'light',\n",
       " 'of',\n",
       " 'day',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 306
    }
   ],
   "source": [
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    " ### Lemmatize "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x]) # Lemmatize every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'is',\n",
       " 'devastating',\n",
       " 'US',\n",
       " 'restaurant',\n",
       " 'chain',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'why',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'company',\n",
       " 'should',\n",
       " 'see',\n",
       " 'the',\n",
       " 'light',\n",
       " 'of',\n",
       " 'day',\n",
       " 'in',\n",
       " 'the',\n",
       " 'near',\n",
       " 'future',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 308
    }
   ],
   "source": [
    "#run example\n",
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    "### Remove Stop Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes default stop_words\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'devastating',\n",
       " 'US',\n",
       " 'restaurant',\n",
       " 'chain',\n",
       " 'But',\n",
       " 'explains',\n",
       " 'financially',\n",
       " 'stronger',\n",
       " 'company',\n",
       " 'see',\n",
       " 'light',\n",
       " 'day',\n",
       " 'near',\n",
       " 'future',\n",
       " 'WSJWhatsNow']"
      ]
     },
     "metadata": {},
     "execution_count": 311
    }
   ],
   "source": [
    "#run example\n",
    "df_raw['clean'].iloc[-1]"
   ]
  },
  {
   "source": [
    "We did not use a stemmer. Lemmatizer was better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Remove any leftover punctuation:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.', '/', ',', '%', '!', ':', \"'s\", \"'ve\", '\"', '*', '‚Äú', \"‚Äô\", \"`\", '?', '‚Äù', \"``\", \"‚Äò\"] \n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6578     [This, health, scare, ha, created, nation, met...\n",
       "16710    [Driven, belief, disruption, progress, cost, I...\n",
       "7764     [If, President, Trump, continues, run, populis...\n",
       "14408    [For, decade, told, desalination, would, one, ...\n",
       "6442     [The, country, wonder, whether, politician, st...\n",
       "7123     [Thousands, Migrants, Attempt, Cross, Europe, ...\n",
       "14387    [If, Turkeys, president, get, way, fourth, tra...\n",
       "12761    [Two, dam, Michigan, breached, day, heavy, rai...\n",
       "8487     [As, lockdown, end, cycling, look, set, helpin...\n",
       "19247    [What, Meryl, Streep, The, Post, teach, u, wom...\n",
       "5402     [Nikolas, share, price, day, investment, firm,...\n",
       "10644    [In, Opinion, Kids, still, miss, sleepover, Sc...\n",
       "2715     [Because, staying, close, home, key, safety, p...\n",
       "2108     [Are, eager, help, shape, The, Economists, dig...\n",
       "12112    [Indigenous, asylumseekers, face, disproportio...\n",
       "8752     [This, animal, liberation, group, actually, wa...\n",
       "7200     [Teenage, brain, run, raw, emotion, Thats, nec...\n",
       "7484     [Chinas, export, picked, momentum, July, secon...\n",
       "3158     [Heres, early, look, front, page, The, Wall, S...\n",
       "19383    [Her, best, known, work, history, financial, f...\n",
       "Name: clean, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 313
    }
   ],
   "source": [
    "df_raw['clean'].sample(20)"
   ]
  },
  {
   "source": [
    "### Remove any additional words like pronouns and other fillers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_words = ['The', \"the\", \"I\", \"he\", \"He\", \"She\", \"she\", \"her\", \"Her\", \"We\", \"we\", \"Me\", \"me\", \"Us\", \"us\", \"it\",\"It\", \"Them\", \"them\", \"They\", \"they\", \"There\", \"there\", \"that\", \"That\", \"This\", \"this\", \"You\", \"you\", \"who\", \"Who\", \"whom\" , \"Whom\", \"whose\", \"Whose\", \"what\", \"What\", \"which\", \"Which\", \"my\", \"My\", \"mine\",\"Mine\", \"your\", \"Your\", \"yours\", \"Yours\", \"his\", \"His\", \"hers\" \"Hers\", \"its\", \"Its\", \"It's\", \"it's\", \"Is\", \"is\" \"was\", \"Was\", \"In\", \"in\", \"across\", \"Across\", \"rather\", \"Rather\", \"roughly\", \"Roughly\", \"why\", \"Why\", \"where\", \"Where\", \"here\", \"Here\", \"A\", \"a\", \"Some\", \"some\", \"few\", \"Few\", \"None\", \"none\", \"more\", \"More\", \"Yes\", \"yes\", \"No\", \"no\", \"If\", \"if\", \"Let\", \"let\", \"Let's\", \"let's\", \"Came\", \"come\", \"go\", \"Go\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "df_raw['clean'] = df_raw['clean'].apply(lambda x: [word for word in x if word not in pronoun_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18307    [Gioncarlo, Valentines, photo, series, Soft, F...\n",
       "19524    [Yet, Massachusetts, like, many, state, ha, co...\n",
       "2618     [past, decade, ha, witnessed, dangerous, trend...\n",
       "17317    [Tracing, killer, Crime, Punishment, tour, St,...\n",
       "5634     [beginning, presidency, unique, logic, charact...\n",
       "8592     [Trump, expected, sign, drug, pricing, executi...\n",
       "7321     [Post, making, analysis, US, climate, data, ac...\n",
       "13227    [coronavirus, pandemic, ha, taken, incalculabl...\n",
       "10342    [never, reduce, risk, infection, zero, But, st...\n",
       "18469    [Mitski, obsessive, fan, growing, independents...\n",
       "12387    [Movies, crossword, topic, contention, Marriag...\n",
       "767      [Moroccan, sex, crime, trial, fuel, fear, crac...\n",
       "5648     [Watch, full, interview, ECB, president, speak...\n",
       "7576     [weekend, brings, Leap, Year, sale, thats, rea...\n",
       "6699     [Trumps, assault, US, Postal, Service, give, D...\n",
       "833      [Borowitz, Report, Donald, Trump, said, much, ...\n",
       "9777     [ECB, push, eurozone, bad, bank, clean, soured...\n",
       "5593     [deleted, earlier, tweet, misspelled, last, na...\n",
       "1902     [Republicans, hope, stimulus, deal, Supreme, C...\n",
       "2286     [United, States, considered, one, stable, demo...\n",
       "Name: clean, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 319
    }
   ],
   "source": [
    "df_raw['clean'].sample(20)"
   ]
  },
  {
   "source": [
    "### Vectorize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['clean_vec'] = df_raw['clean'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                           Atlantic Daily Will decade new\n",
       "1        Theres plenty thats going wrong Trump four thi...\n",
       "2        Trump try steal election people surely protest...\n",
       "3        Trump campaign electionsecurity operation key ...\n",
       "4        Even Joe Biden win decisively next week coming...\n",
       "                               ...                        \n",
       "19995    General Mills make Cheerios cereal Yoplait yog...\n",
       "19996    From When viral panic subsides Fed rethink mor...\n",
       "19997            government want send moneybut soon arrive\n",
       "19998    frozen asparagus said one disappointed shopper...\n",
       "19999    coronavirus pandemic devastating US restaurant...\n",
       "Name: clean_vec, Length: 200000, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 321
    }
   ],
   "source": [
    "df_raw['clean_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = tfidf.fit_transform(df_raw['clean_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc_term_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    __  ___  ____  _____  _______________________  __________________________  \\\n",
       "0  0.0  0.0   0.0    0.0                      0.0                         0.0   \n",
       "1  0.0  0.0   0.0    0.0                      0.0                         0.0   \n",
       "2  0.0  0.0   0.0    0.0                      0.0                         0.0   \n",
       "3  0.0  0.0   0.0    0.0                      0.0                         0.0   \n",
       "4  0.0  0.0   0.0    0.0                      0.0                         0.0   \n",
       "\n",
       "   _________________________________  __zz  _zz  _„ÉÑ_  ...  Ô¨Çora  Ô¨Çow  Ô¨ÇuÔ¨Äy  \\\n",
       "0                                0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0   \n",
       "1                                0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0   \n",
       "2                                0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0   \n",
       "3                                0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0   \n",
       "4                                0.0   0.0  0.0  0.0  ...   0.0  0.0   0.0   \n",
       "\n",
       "    Ô¨Çy  Ô¨Çygskam  Ô¨Çying  ùêÄùêØùêöùê¢ùê•ùêöùêõùê•ùêû  ùêîùê©ùêùùêöùê≠ùêûùê¨  ùòßùò∞ùò≥  ùò©ùò™ùòÆ  \n",
       "0  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "1  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "2  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "3  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "4  0.0      0.0    0.0        0.0      0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 77504 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>__</th>\n      <th>___</th>\n      <th>____</th>\n      <th>_____</th>\n      <th>_______________________</th>\n      <th>__________________________</th>\n      <th>_________________________________</th>\n      <th>__zz</th>\n      <th>_zz</th>\n      <th>_„ÉÑ_</th>\n      <th>...</th>\n      <th>Ô¨Çora</th>\n      <th>Ô¨Çow</th>\n      <th>Ô¨ÇuÔ¨Äy</th>\n      <th>Ô¨Çy</th>\n      <th>Ô¨Çygskam</th>\n      <th>Ô¨Çying</th>\n      <th>ùêÄùêØùêöùê¢ùê•ùêöùêõùê•ùêû</th>\n      <th>ùêîùê©ùêùùêöùê≠ùêûùê¨</th>\n      <th>ùòßùò∞ùò≥</th>\n      <th>ùò©ùò™ùòÆ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 77504 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 325
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['lucretiuss', 'lucy', 'ludic', 'ludicrous', 'ludicrously',\n",
       "       'ludicrousness', 'ludicrousshe', 'ludlow', 'ludosky', 'ludovic',\n",
       "       ...\n",
       "       'marcel', 'marcela', 'marcelino', 'marcella', 'marcellis', 'marcelo',\n",
       "       'march', 'marcha', 'marchand', 'marcharse'],\n",
       "      dtype='object', length=1000)"
      ]
     },
     "metadata": {},
     "execution_count": 326
    }
   ],
   "source": [
    "df.columns[40000:41000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"doc_term_matrix_0.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        X = super().transform(raw_documents, copy=copy)\n",
    "        df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = super().fit_transform(raw_documents, y=y)\n",
    "        df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "        return df"
   ]
  }
 ]
}